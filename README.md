# Clarifai Go Agent Runtime

This project implements a Go-based agent runtime designed to be compatible with the Agent-to-Agent (A2A) communication protocol. It provides both a command-line interface (`ai`) for direct interaction with a configured LLM and an HTTP server exposing A2A-compliant endpoints for task management and interaction.

The primary goal is to create a flexible and extensible runtime that can manage tasks, interact with LLMs (initially via LM Studio's OpenAI-compatible API), and potentially integrate with other tools and capabilities in the future.

## Features

*   **A2A HTTP Server:**
    *   Serves agent self-description at `/.well-known/agent.json`.
    *   Implements core A2A task endpoints:
        *   `/tasks/send`: Accepts tasks for asynchronous processing.
        *   `/tasks/sendSubscribe`: Accepts tasks and streams responses via Server-Sent Events (SSE).
        *   `/tasks/status`: Retrieves the status and details of a task.
        *   `/tasks/input`: Allows providing input to tasks waiting in the `input-required` state.
        *   `/tasks/artifact`: Retrieves artifacts generated by tasks.
        *   `/tasks/pushNotification/set`: Placeholder for push notification registration.
    *   Supports different input `Part` types (`TextPart`, `FilePart`, `DataPart`) for task submission.
        *   Basic handling for `file://` URIs in `FilePart` is included.
    *   Handles `input-required` state transitions based on LLM response markers.
*   **Task Management:**
    *   Defines a `Task` model with states (`submitted`, `working`, `input-required`, `completed`, `failed`, `canceled`).
    *   Includes both an `InMemoryTaskStore` (default, non-persistent) and a `FileTaskStore` (persistent, saves tasks as JSON files in `_tasks/`).
*   **`ai` Command-Line Tool:**
    *   Provides direct interaction with the configured LLM (requires LM Studio running).
    *   Supports piping input (`cat file | ai`).
    *   Supports streaming responses (`ai --stream "prompt"`).
    *   Can output the agent's self-description (`ai --describe`).
*   **LLM Integration:**
    *   Connects to OpenAI-compatible APIs (tested with LM Studio).
    *   Configurable via environment variables (see `llm/llm.go`).
    *   Supports streaming responses from the LLM.

## Getting Started

### Prerequisites

*   Go (version 1.22 or later recommended)
*   [LM Studio](https://lmstudio.ai/) installed and running (`lms server start`) for LLM interaction.

### Building

```bash
make build
```
This creates the `clarifai-agent` executable in the project root.

### Running the A2A Server

```bash
./clarifai-agent server
# Or using the default command:
# ./clarifai-agent
```
This starts the HTTP server (defaulting to port 8080) using the `InMemoryTaskStore`.

To use the persistent `FileTaskStore`:
```bash
./clarifai-agent server --task-store file --task-store-path ./my_tasks
```

The server exposes endpoints like:
*   `http://localhost:8080/.well-known/agent.json`
*   `http://localhost:8080/tasks/send` (POST)
*   `http://localhost:8080/tasks/sendSubscribe` (POST)
*   `http://localhost:8080/tasks/status?id={task_id}` (GET)
*   `http://localhost:8080/tasks/input` (POST)
*   ... and others.

### Using the `ai` CLI Tool

Ensure the `clarifai-agent` binary is in your PATH or use the full path.

**Basic Interaction:**
```bash
./clarifai-agent ai "Hello, world!"
```

**Streaming:**
```bash
./clarifai-agent ai --stream "Tell me a story."
```

**Piping:**
```bash
cat README.md | ./clarifai-agent ai "Summarize this file."
```

**Agent Description:**
```bash
./clarifai-agent ai --describe
```

## Configuration

LLM connection details (API key, base URL, model) can be configured via environment variables (e.g., `LLM_API_KEY`, `LLM_API_BASE`, `LLM_MODEL`). See `llm/llm.go` for details.

## Development

*   **Code Structure:**
    *   `main.go`: Entrypoint for CLI commands and server startup.
    *   `http.go`: A2A HTTP server setup and agent card definition.
    *   `ai.go`: Implementation of the `ai` CLI command.
    *   `a2a/`: Package containing A2A protocol types (Task, Message, Part, Artifact), TaskStore interface/implementations, and HTTP handlers.
    *   `llm/`: Package for LLM client abstraction and interaction.
*   **Building:** `make build`
*   **Testing:** `make test` (or `go test ./...`)
*   **TODO:** See `TODO.md` for the current implementation status and planned features.

## Future Work

*   Implement task resumption logic for `input-required` state.
*   Full implementation of push notifications.
*   Support for more `Part` types and URI schemes (e.g., `http://`, `data://`).
*   Integration with MCP tools.
*   More robust error handling and input validation.
*   Comprehensive unit and integration tests.
*   Containerization improvements (health checks, etc.).
